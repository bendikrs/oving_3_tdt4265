{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Report - Group 14\n",
    "Kristoffer Norli & Bendik Stokke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## task 1a)\n",
    "For task 1a) we chose to handle the boundary conditions by adding a one pixel wide padding of zeros around the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pdfs_and_images/1a_f.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pdfs_and_images/1g.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## 2 a)\n",
    "![](plots/task2_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 b)\n",
    "Final validation accuracy   72,21%      \n",
    "Final train accuracy        87,03%     \n",
    "Final test accuracy         73,11%     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 a)\n",
    "Model 1\n",
    "\n",
    "![](pdfs_and_images/model1_arkitektur.png) \n",
    "\n",
    "Batch size: 50, Learning rate: 0.02     \n",
    "Optimizer: Average SGD, weight decay = 0.001, Weight init: Xavier uniform   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Model 2\n",
    "\n",
    "![](pdfs_and_images/model2_arkitektur.png)   \n",
    "Batch size: 64, Learning rate: 0.05      \n",
    "Optimizer: Average SGD, weight decay = 0.001 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 b)\n",
    "![](pdfs_and_images/task3_model1_best_plot.png)\n",
    "![](pdfs_and_images/table_models.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 c)\n",
    "We saw a big impact from adjust the learning rate. A too high learning rate cause very fast training but bad accuracy and high loss. It seemed also like the network overfitted more with a high learning rate. Lowering the learning rate made the network perform better in terms of both accuracy and loss, but it then used more epochs to converge.   \n",
    "\n",
    "We tried many things which did not work that well. We tried using adaptive average pooling, using batch normalization on all layers, data augmentation with random brightness, and strided convolutions instead of pooling layers. We also tried using the Adam optimizer, with a weight decay of 0, but then the accuracy dropped drasticly.\n",
    "\n",
    "We also tried using a lot of dropout with different probability, and from a test with dropout on nearly every convolution iteration we observed a significant drop in test accuracy. This may be due to the fact that it essentially removes too many nodes from the network so that it in the end won't have \"learned\" as much as it should. It is also worth mentioning that with a lot of dropout we saw a lower effect of overfitting. This was to be expected, as the main task for dropout is to reduce overfitting.\n",
    "\n",
    "\n",
    "## 3 d)\n",
    "![](pdfs_and_images/comparison_3d.png)\n",
    "\n",
    "## 3 e)\n",
    "![](pdfs_and_images/task3_model1_best.png)  \n",
    "Final validation accuracy 81,559%   \n",
    "Final train accuracy      90,460%   \n",
    "Final test accuracy       81,299%   \n",
    "Final train loss           0,5346   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 f)\n",
    "We see that the train accuracy is a lot higher than the test accuracy, 90.4% versus 81.3%. That is a sign of some overfitting as the network performs better on the training dataset than the \"unseen\" test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## 4 a)\n",
    "![](pdfs_and_images/task4a.png)   \n",
    "We used the Adam optimizer\n",
    "Hyperparameters:\n",
    "batch_size    = 32   \n",
    "learning_rate = 0.0005\n",
    "\n",
    "Final test accuracy: 89,96%\n",
    "\n",
    "## 4 b)\n",
    "![](pdfs_and_images/task4b.png)   \n",
    "We see that filter 14 looks for vertical lines, filter 26 for horizontal lines and filter 49 looks for diagonal lines in the image. Filter 32 and 52 looks like it tries to get the contour of the zebra by separating it from the background. \n",
    "\n",
    "## 4 c)\n",
    "![](pdfs_and_images/task4c.png)   \n",
    "At the last layer, the activations are very small only 7x7 pixels. So we can see that the features the filters extract very specific things in the image, that seems very random.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
