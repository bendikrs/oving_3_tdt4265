{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Report - Group 14\n",
    "Kristoffer Norli & Bendik Stokke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## task 1a)\n",
    "For task 1a) we chose to handle the boundary conditions by adding a one pixel wide padding of zeros around the image.\n",
    "![](pdfs_and_images/1a_f.jpg)"
   ]
  },
  {
   "source": [
    "![](pdfs_and_images/1g.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Number of parameters = 784x64 + 64x10 + 1x64 = 50816 weights + 64 biases = 50880 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3_alt.png)\n",
    "We can see that by using an improved weight initializing, we get a significant higher validation accuracy of ~95.8% instead of the ~91.7% in the base model. The model then also converges to max accuracy almost by only 10000 training steps.\n",
    "\n",
    "When we additionally use the improved sigmoid, we get 96.4% accuracy, a improvement of ~1.5% from using only improved weight initializing. We see that this addition of the improved sigmoid gives a significant increase in convergence speed, as the loss quickly reaches stable low values.\n",
    "Also we observe that the early stopping kick in quite early, stopping at about 12500 training steps.\n",
    "\n",
    "The addition of the momentum algorithm reduces the number of needed training steps additionally, but it does not improve accuracy that much.\n",
    "\n",
    "The training accuracy converges much faster than the validation accuracy in all cases, which is a typical sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "![](task_4_a.png)\n",
    "The network uses more training steps to converge, using ~17000 steps instead of ~11000 steps with 64 hidden units. Also the model don't reach the same level of accuracy, topping off at ~95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "![](task_4_b.png)\n",
    "The network uses more training steps to converge, using ~12000 steps instead of ~11000 steps with 64 hidden units. The 128 hidden unit model reaches a higher level of accuracy, topping off at ~96.8%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task_4_d.png)\n",
    "We used 60 nodes in the two hidden layers, resulting in 60+60=120 hidden units.\n",
    "Then the number of parameters is: 784x60 + 60x60 + 60x10 + 1x60 = 51240 weights + 60 biases = 51300 parameters which is 420 parameters more than the model in task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task_4_e.png)\n",
    "The 10 layer model has a worse accuracy of ~95.2% instead of the of ~96.2% accuracy of the model with one layer. We also see that the 10-layer moodel converges much more inconsistent and slower than the base model. We think this maybe is because the model is too complex (have to many parameters) for this relatively simple task of recognizing handwritten numbers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}